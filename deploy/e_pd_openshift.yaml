---
# 1. SHARED STORAGE
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ec-cache-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi

---
# 2. INTERNAL SERVICES
apiVersion: v1
kind: Service
metadata:
  name: vllm-encoder
spec:
  selector:
    app: vllm-encoder
  ports:
    - protocol: TCP
      port: 8001
      targetPort: 8001
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-pd
spec:
  selector:
    app: vllm-pd
  ports:
    - protocol: TCP
      port: 8002
      targetPort: 8002
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-proxy
spec:
  selector:
    app: vllm-proxy
  ports:
    - protocol: TCP
      port: 8000
      targetPort: 8000

---
# 3. ENCODER DEPLOYMENT (2 Replicas + Permission Fix)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-encoder
spec:
  replicas: 2
  selector:
    matchLabels:
      app: vllm-encoder
  template:
    metadata:
      labels:
        app: vllm-encoder
    spec:
      initContainers:
        - name: fix-permissions
          image: busybox:latest
          # Changes ownership/permissions so the random OpenShift UID can write
          command: ["sh", "-c", "chmod -R 777 /shared-ec-cache"]
          volumeMounts:
            - name: ec-cache
              mountPath: /shared-ec-cache
      containers:
        - name: vllm
          image: ghcr.io/revit13/vllm-cpu-env:latest
          # image: vllm/vllm-openai:latest # The standard vllm-openai image is a CUDA-specialized build.
          env:
            - name: VLLM_TARGET_DEVICE
              value: "cpu"
            - name: VLLM_USE_V1
              value: "0"
          command: ["python3", "-m", "vllm.entrypoints.openai.api_server"]
          args:
            - "--model=Qwen/Qwen3-VL-2B-Instruct"
            - "--port=8001"
            - "--enforce-eager"
            # - "--device=cpu"  # CRITICAL: Explicitly set device to CPU
            - "--no-enable-prefix-caching"
            - "--mm-encoder-only"
            - "--ec-transfer-config"
            - '{"ec_connector": "ECExampleConnector", "ec_role": "ec_producer", "ec_connector_extra_config": {"shared_storage_path": "/shared-ec-cache"}}'
          volumeMounts:
            - mountPath: /shared-ec-cache
              name: ec-cache
      volumes:
        - name: ec-cache
          persistentVolumeClaim:
            claimName: ec-cache-pvc

---
# 4. PREFILL/DECODE DEPLOYMENT (Full CPU Fallback)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-pd
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-pd
  template:
    metadata:
      labels:
        app: vllm-pd
    spec:
      nodeSelector:
        nvidia.com/gpu.present: "true"
      initContainers:
        - name: fix-permissions
          image: busybox:latest
          command: ["sh", "-c", "chmod -R 777 /shared-ec-cache"]
          volumeMounts:
            - name: ec-cache
              mountPath: /shared-ec-cache
      containers:
        - name: vllm
          image: vllm/vllm-openai:latest
          env:
            - name: VLLM_USE_V1
              value: "1"  
            - name: VLLM_TARGET_DEVICE
              value: "cuda"
          command: ["python3", "-m", "vllm.entrypoints.openai.api_server"]
          args:
            - "--model=Qwen/Qwen3-VL-2B-Instruct"
            - "--port=8002"
            - "--trust-remote-code"
            - "--enforce-eager"
            - "--gpu-memory-utilization=0.7"
            - "--ec-transfer-config"
            - '{"ec_connector": "ECExampleConnector", "ec_role": "ec_consumer", "ec_connector_extra_config": {"shared_storage_path": "/shared-ec-cache"}}'
          resources:
            limits:
              nvidia.com/gpu: 1
              cpu: "8"
              memory: "30Gi"
            requests:
              cpu: "4"
              memory: "30Gi"
          volumeMounts:
            - mountPath: /shared-ec-cache
              name: ec-cache
      volumes:
        - name: ec-cache
          persistentVolumeClaim:
            claimName: ec-cache-pvc

---
# 5. EPD PROXY (OpenShift Compatible)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-proxy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-proxy
  strategy:
    type: Recreate # Kills old pod before starting new one to release H100
  template:
    metadata:
      labels:
        app: vllm-proxy
    spec:
      terminationGracePeriodSeconds: 60 # Gives NCCL time to close sockets
      containers:
        - name: proxy
          image: python:3.10-slim
          workingDir: /tmp
          env:
            - name: HOME
              value: /tmp
            - name: PYTHONUSERBASE
              value: /tmp/.local
          command: ["/bin/bash", "-c"]
          args:
            - |
              export PATH=$PATH:/tmp/.local/bin
              pip install --user aiohttp fastapi "uvicorn[standard]" uvloop
              python3 -c "import urllib.request; urllib.request.urlretrieve('https://raw.githubusercontent.com/vllm-project/vllm/main/examples/online_serving/disaggregated_encoder/disagg_epd_proxy.py', 'disagg_epd_proxy.py')"
              python3 disagg_epd_proxy.py \
                --port 8000 \
                --encode-servers-urls http://vllm-encoder:8001 \
                --prefill-servers-urls disable \
                --decode-servers-urls http://vllm-pd:8002
          ports:
            - containerPort: 8000
